{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc = pd.read_parquet(\"/Users/jiang/Desktop/fhvhv_tripdata_2024-01.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickUpID = tlc['PULocationID']\n",
    "dropOffID = tlc['DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_nta = pd.read_csv(\"taxi-zone-lookup-with-ntacode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_df = tlc.merge(tlc_nta, how=\"left\", left_on='PULocationID', right_on=\"location_id\")\n",
    "do_df = tlc.merge(tlc_nta, how=\"left\", left_on='DOLocationID', right_on=\"location_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nta = pd.read_csv(\"Neighborhood_Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes all elements in column '% Other' to be floats by removing the % symbol\n",
    "nta['% Other'] = nta[\"% Other\"].str[:-1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrgd = tlc.merge(tlc_nta, how=\"left\", left_on='PULocationID', right_on=\"location_id\", suffixes=(None, \"PU\"))\n",
    "mrgd = mrgd.merge(tlc_nta, how=\"left\", left_on='DOLocationID', right_on=\"location_id\", suffixes=(None, \"DO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrgd = mrgd.merge(nta, how=\"left\", left_on='ntacode', right_on=\"NTA Code\", suffixes=(None, \"PU\"))\n",
    "mrgd = mrgd.merge(nta, how=\"left\", left_on='ntacodeDO', right_on=\"NTA Code\", suffixes=(None, \"DO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num',\n",
       "       'request_datetime', 'on_scene_datetime', 'pickup_datetime',\n",
       "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
       "       'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
       "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
       "       'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
       "       'wav_request_flag', 'wav_match_flag', 'location_id', 'borough', 'zone',\n",
       "       'service_zone', 'ntacode', 'location_idDO', 'boroughDO', 'zoneDO',\n",
       "       'service_zoneDO', 'ntacodeDO',\n",
       "       'Neighborhood Tabulation Area (NTA) Name', 'NTA Code', 'Boro Name',\n",
       "       'Boro CD', 'Total Population', '65+ years', '%65+ yeras',\n",
       "       '%65+ Below poverty', '% Hispanic/Latino', '% White',\n",
       "       '% Black/African American', '% Asian', '% Other',\n",
       "       'Neighborhood Tabulation Area (NTA) NameDO', 'NTA CodeDO',\n",
       "       'Boro NameDO', 'Boro CDDO', 'Total PopulationDO', '65+ yearsDO',\n",
       "       '%65+ yerasDO', '%65+ Below povertyDO', '% Hispanic/LatinoDO',\n",
       "       '% WhiteDO', '% Black/African AmericanDO', '% AsianDO', '% OtherDO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrgd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrgd = mrgd.drop(columns=['dispatching_base_num', 'originating_base_num',\\\n",
    "                          'location_id', 'service_zone', 'ntacode',\\\n",
    "                          'location_idDO', 'service_zoneDO', 'ntacodeDO',\\\n",
    "                          'NTA Code', 'NTA CodeDO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrgd = mrgd.rename(columns={'hvfhs_license_num': 'Ride-Hailing Service Number',\\\n",
    "                            'request_datetime': 'Request Datetime',\\\n",
    "                            'on_scene_datetime': 'On-Scene Datetime',\\\n",
    "                            'pickup_datetime': 'Pick-Up Datetime',\\\n",
    "                            'dropoff_datetime': 'Drop-Off Datetime',\\\n",
    "                            'PULocationID': 'Pick-Up Location ID',\\\n",
    "                            'DOLocationID': 'Drop-Off Location ID',\\\n",
    "                            'trip_miles': 'Trip Distance (miles)',\\\n",
    "                            'trip_time': 'Trip Length (seconds)',\\\n",
    "                            'base_passenger_fare': 'Base Passenger Fare',\\\n",
    "                            'tolls': 'Toll Fare',\\\n",
    "                            'bcf': 'Black Car Fund Fare',\\\n",
    "                            'sales_tax': 'NYS Sales Tax Fare',\\\n",
    "                            'congestion_surcharge': 'NYS Congestion Surcharge',\\\n",
    "                            'airport_fee': 'Airport Fee',\\\n",
    "                            'tips': 'Tips Given',\\\n",
    "                            'driver_pay': 'Driver Pay',\\\n",
    "                            'shared_request_flag': 'Passenger Agreed to Share Ride', \\\n",
    "                            'shared_match_flag': 'Passenger Shared Ride',\\\n",
    "                            'access_a_ride_flag': 'Administered by MTA',\\\n",
    "                            'wav_request_flag': 'Passenger Requested WAV',\\\n",
    "                            'wav_match_flag': 'Passenger Rode in WAV'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ride-Hailing Service Number', 'Request Datetime', 'On-Scene Datetime',\n",
       "       'Pick-Up Datetime', 'Drop-Off Datetime', 'Pick-Up Location ID',\n",
       "       'Drop-Off Location ID', 'Trip Distance (miles)',\n",
       "       'Trip Length (seconds)', 'Base Passenger Fare', 'Toll Fare',\n",
       "       'Black Car Fund Fare', 'NYS Sales Tax Fare', 'NYS Congestion Surcharge',\n",
       "       'Airport Fee', 'Tips Given', 'Driver Pay',\n",
       "       'Passenger Agreed to Share Ride', 'Passenger Shared Ride',\n",
       "       'Administered by MTA', 'Passenger Requested WAV',\n",
       "       'Passenger Rode in WAV', 'borough', 'zone', 'boroughDO', 'zoneDO',\n",
       "       'Neighborhood Tabulation Area (NTA) Name', 'Boro Name', 'Boro CD',\n",
       "       'Total Population', '65+ years', '%65+ yeras', '%65+ Below poverty',\n",
       "       '% Hispanic/Latino', '% White', '% Black/African American', '% Asian',\n",
       "       '% Other', 'Neighborhood Tabulation Area (NTA) NameDO', 'Boro NameDO',\n",
       "       'Boro CDDO', 'Total PopulationDO', '65+ yearsDO', '%65+ yerasDO',\n",
       "       '%65+ Below povertyDO', '% Hispanic/LatinoDO', '% WhiteDO',\n",
       "       '% Black/African AmericanDO', '% AsianDO', '% OtherDO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrgd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/jgwmbyqx15scf95nddcnzm040000gn/T/ipykernel_76409/3581375618.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mrgd_copy['high_fare'] = (mrgd_copy['Base Passenger Fare'] > fare_median).astype(int)\n",
      "/var/folders/1v/jgwmbyqx15scf95nddcnzm040000gn/T/ipykernel_76409/3581375618.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mrgd_copy['protected_black'] = (mrgd_copy['% Black/African American'] > eth_median).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Preprocessing\n",
    "mrgd_copy = mrgd\n",
    "\n",
    "# Drop rows with missing values (or apply your preferred imputation)\n",
    "mrgd_copy = mrgd_copy.dropna()\n",
    "\n",
    "# Create a binary target variable.\n",
    "# classify a ride as “high fare” if Base Passenger Fare > the median.\n",
    "fare_median = mrgd_copy['Base Passenger Fare'].median()\n",
    "mrgd_copy['high_fare'] = (mrgd_copy['Base Passenger Fare'] > fare_median).astype(int)\n",
    "\n",
    "# protected attribute.\n",
    "# create a binary attribute: 1 if the value is above the median, 0 otherwise.\n",
    "eth_median = mrgd_copy['% Black/African American'].median()\n",
    "mrgd_copy['protected_black'] = (mrgd_copy['% Black/African American'] > eth_median).astype(int)\n",
    "\n",
    "#eth_hispanic_median = mrgd_copy['% Hispanic/Latino'].median()\n",
    "#mrgd_copy['protected_hispanic'] = (mrgd_copy['% Hispanic/Latino'] > eth_hispanic_median).astype(int)\n",
    "\n",
    "#eth_hispanic_median = mrgd_copy['% Asian'].median()\n",
    "#mrgd_copy['protected_asian'] = (mrgd_copy['% Asian/Latino'] > eth_asian_median).astype(int)\n",
    "\n",
    "\n",
    "#  \"Trip Distance (miles)\" and \"Trip Length (seconds)\" as features\n",
    "features = ['Trip Distance (miles)', 'Trip Length (seconds)']\n",
    "X = mrgd_copy[features]\n",
    "y = mrgd_copy['high_fare']\n",
    "A = mrgd_copy['protected_black']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "    X, y, A, test_size=0.3, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Logistic Regression Model ===\n",
      "Baseline Accuracy: 0.878\n",
      "Baseline Error Rates by Protected Group (0: lower, 1: higher '% Black'):\n",
      "  Group 0: FPR = 0.072, FNR = 0.198\n",
      "  Group 1: FPR = 0.122, FNR = 0.080\n"
     ]
    }
   ],
   "source": [
    "#2. Baseline\n",
    "print(\"=== Baseline Logistic Regression Model ===\")\n",
    "baseline_clf = LogisticRegression(solver='liblinear')\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set\n",
    "y_pred_baseline = baseline_clf.predict(X_test)\n",
    "\n",
    "acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Accuracy: {acc_baseline:.3f}\")\n",
    "\n",
    "# Function to compute error rates per group (False Positive and False Negative rates)\n",
    "def compute_error_rates(y_true, y_pred, sensitive):\n",
    "    groups = np.unique(sensitive)\n",
    "    rates = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive == g)\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred[mask]\n",
    "        # Confusion matrix components\n",
    "        tp = np.sum((y_true_g == 1) & (y_pred_g == 1))\n",
    "        tn = np.sum((y_true_g == 0) & (y_pred_g == 0))\n",
    "        fp = np.sum((y_true_g == 0) & (y_pred_g == 1))\n",
    "        fn = np.sum((y_true_g == 1) & (y_pred_g == 0))\n",
    "        # Avoid division by zero\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "        rates[g] = {'FPR': fpr, 'FNR': fnr}\n",
    "    return rates\n",
    "\n",
    "baseline_rates = compute_error_rates(y_test.values, y_pred_baseline, A_test.values)\n",
    "print(\"Baseline Error Rates by Protected Group (0: lower, 1: higher '% Black'):\")\n",
    "for group, rates in baseline_rates.items():\n",
    "    print(f\"  Group {group}: FPR = {rates['FPR']:.3f}, FNR = {rates['FNR']:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model\n",
    "Overall Accuracy: 87.8%\n",
    "\n",
    "Error Rates by Protected Group:\n",
    "Group 0 (Lower '% Black'):\n",
    "False Positive Rate (FPR): 7.2%\n",
    "False Negative Rate (FNR): 19.8%\n",
    "Group 1 (Higher '% Black'):\n",
    "FPR: 12.2%\n",
    "FNR: 8.0%\n",
    "\n",
    "The baseline model has a noticeable disparity between groups:\n",
    "FPR Disparity: Group 1 experiences a higher FPR compared to Group 0, meaning that members of Group 1 are more likely to be incorrectly classified as having a “high fare” when they actually do not.\n",
    "FNR Disparity: Conversely, Group 0 has a higher FNR, meaning they are more likely to be incorrectly classified as “low fare” when they should be “high fare.”\n",
    "This imbalance suggests that the model might be favoring one group over the other in terms of the types of errors it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fairness-Aware Model (Equalized Odds via ExponentiatedGradient) ===\n",
      "Fair Model Accuracy: 0.766\n",
      "Fair Model Error Rates by Protected Group:\n",
      "  Group 0: FPR = 0.177, FNR = 0.298\n",
      "  Group 1: FPR = 0.176, FNR = 0.282\n"
     ]
    }
   ],
   "source": [
    "#3. Fairness-Aware Mitigation with Equalized Odds\n",
    "print(\"\\n=== Fairness-Aware Model (Equalized Odds via ExponentiatedGradient) ===\")\n",
    "# Set fairness constraint\n",
    "#Fairlearn EqualizedOdds: ensures that the false positive and false negative rates are similar across groups.\n",
    "constraint = EqualizedOdds()\n",
    "\n",
    "# ExponentiatedGradient to enforce the constraint.\n",
    "fair_clf = ExponentiatedGradient(\n",
    "    estimator=LogisticRegression(solver='liblinear'),\n",
    "    constraints=constraint,\n",
    "    eps=0.01  #play around\n",
    ")\n",
    "\n",
    "# Fit fairness-aware model\n",
    "fair_clf.fit(X_train, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_pred_fair = fair_clf.predict(X_test)\n",
    "acc_fair = accuracy_score(y_test, y_pred_fair)\n",
    "print(f\"Fair Model Accuracy: {acc_fair:.3f}\")\n",
    "\n",
    "# Compute error rates for fair model\n",
    "fair_rates = compute_error_rates(y_test.values, y_pred_fair, A_test.values)\n",
    "print(\"Fair Model Error Rates by Protected Group:\")\n",
    "for group, rates in fair_rates.items():\n",
    "    print(f\"  Group {group}: FPR = {rates['FPR']:.3f}, FNR = {rates['FNR']:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness vs. Accuracy Trade-Off:\n",
    "\n",
    "Baseline Model: High accuracy but unequal error distribution between groups.\n",
    "\n",
    "Fairness-Aware Model: Lower overall accuracy but much more balanced error rates across groups.\n",
    "\n",
    "Implications for Price Discrimination Bias:\n",
    "\n",
    "If the goal is to mitigate price discrimination bias, equalizing error rates (i.e., ensuring that neither group is disproportionately misclassified) may be more important than achieving the highest possible accuracy.\n",
    "The fairness-aware model ensures that neither group is unfairly burdened by a higher rate of false positives or false negatives, which is a step toward reducing bias in decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/jgwmbyqx15scf95nddcnzm040000gn/T/ipykernel_76409/2646786463.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mrgd_test['high_fare'] = (mrgd_test['Base Passenger Fare'] > fare_median).astype(int)\n",
      "/var/folders/1v/jgwmbyqx15scf95nddcnzm040000gn/T/ipykernel_76409/2646786463.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mrgd_test['protected_black'] = (mrgd_test['% Black/African American'] > eth_median).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair Model Accuracy: 0.766\n",
      "Fair Model Error Rates by Protected Group:\n",
      "  Group 0: FPR = 0.178, FNR = 0.299\n",
      "  Group 1: FPR = 0.177, FNR = 0.282\n"
     ]
    }
   ],
   "source": [
    "mrgd_test = mrgd\n",
    "mrgd_test = mrgd_test.dropna()\n",
    "\n",
    "fare_median = mrgd_test['Base Passenger Fare'].median()\n",
    "mrgd_test['high_fare'] = (mrgd_test['Base Passenger Fare'] > fare_median).astype(int)\n",
    "\n",
    "eth_median = mrgd_test['% Black/African American'].median()\n",
    "mrgd_test['protected_black'] = (mrgd_test['% Black/African American'] > eth_median).astype(int)\n",
    "\n",
    "\n",
    "features = ['Trip Distance (miles)', 'Trip Length (seconds)']\n",
    "X = mrgd_test[features]\n",
    "y = mrgd_test['high_fare']\n",
    "A = mrgd_test['protected_black']  # Protected attribute\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "    X, y, A, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "constraint = EqualizedOdds()\n",
    "base_estimator = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Initialize the ExponentiatedGradient algorithm.\n",
    "# The 'eps' parameter sets the tolerance for constraint violations.\n",
    "fair_clf = ExponentiatedGradient(estimator=base_estimator, constraints=constraint, eps=0.01)\n",
    "\n",
    "fair_clf.fit(X_train, y_train, sensitive_features=A_train)\n",
    "y_pred_fair = fair_clf.predict(X_test)\n",
    "accuracy_fair = accuracy_score(y_test, y_pred_fair)\n",
    "print(f\"Fair Model Accuracy: {accuracy_fair:.3f}\")\n",
    "def compute_error_rates(y_true, y_pred, sensitive):\n",
    "    groups = np.unique(sensitive)\n",
    "    rates = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive == g)\n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        tp = np.sum((y_true_group == 1) & (y_pred_group == 1))\n",
    "        tn = np.sum((y_true_group == 0) & (y_pred_group == 0))\n",
    "        fp = np.sum((y_true_group == 0) & (y_pred_group == 1))\n",
    "        fn = np.sum((y_true_group == 1) & (y_pred_group == 0))\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "        rates[g] = {'FPR': fpr, 'FNR': fnr}\n",
    "    return rates\n",
    "\n",
    "fair_error_rates = compute_error_rates(y_test.values, y_pred_fair, A_test.values)\n",
    "print(\"Fair Model Error Rates by Protected Group:\")\n",
    "for group, metrics in fair_error_rates.items():\n",
    "    print(f\"  Group {group}: FPR = {metrics['FPR']:.3f}, FNR = {metrics['FNR']:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy has dropped to 76.6% from what might have been a higher baseline accuracy (typically seen when no fairness constraints are applied). This is a common trade-off: by enforcing fairness (i.e., equalizing error rates), the model sacrifices some predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating for protected attribute: Black (protected_black)\n",
      "Baseline Accuracy: 0.878\n",
      "Fair Model Accuracy: 0.766\n",
      "\n",
      "Evaluating for protected attribute: Hispanic (protected_hispanic)\n",
      "Baseline Accuracy: 0.878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 86\u001b[0m\n\u001b[1;32m     80\u001b[0m constraint \u001b[39m=\u001b[39m EqualizedOdds()\n\u001b[1;32m     81\u001b[0m fair_clf \u001b[39m=\u001b[39m ExponentiatedGradient(\n\u001b[1;32m     82\u001b[0m     estimator\u001b[39m=\u001b[39mLogisticRegression(solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     83\u001b[0m     constraints\u001b[39m=\u001b[39mconstraint,\n\u001b[1;32m     84\u001b[0m     eps\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m  \u001b[39m# Adjust as needed\u001b[39;00m\n\u001b[1;32m     85\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m fair_clf\u001b[39m.\u001b[39;49mfit(X_train, y_train, sensitive_features\u001b[39m=\u001b[39;49mA_train)\n\u001b[1;32m     87\u001b[0m y_pred_fair \u001b[39m=\u001b[39m fair_clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     89\u001b[0m fair_acc \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred_fair)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/fairlearn/reductions/_exponentiated_gradient/exponentiated_gradient.py:200\u001b[0m, in \u001b[0;36mExponentiatedGradient.fit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m     gap_LP \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# saddle point optimization over the convex hull of\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39m# classifiers returned so far\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     Q_LP, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_vecs_LP_[t], result_LP \u001b[39m=\u001b[39m lagrangian\u001b[39m.\u001b[39;49msolve_linprog(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu)\n\u001b[1;32m    201\u001b[0m     gap_LP \u001b[39m=\u001b[39m result_LP\u001b[39m.\u001b[39mgap()\n\u001b[1;32m    203\u001b[0m \u001b[39m# keep values from exponentiated gradient or linear programming\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:193\u001b[0m, in \u001b[0;36m_Lagrangian.solve_linprog\u001b[0;34m(self, nu)\u001b[0m\n\u001b[1;32m    188\u001b[0m lambda_vec \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(result_dual\u001b[39m.\u001b[39mx[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstraints\u001b[39m.\u001b[39mindex)\n\u001b[1;32m    189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_linprog_n_hs \u001b[39m=\u001b[39m n_hs\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_linprog_result \u001b[39m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m     Q,\n\u001b[1;32m    192\u001b[0m     lambda_vec,\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_gap(Q, lambda_vec, nu),\n\u001b[1;32m    194\u001b[0m )\n\u001b[1;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_linprog_result\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:148\u001b[0m, in \u001b[0;36m_Lagrangian.eval_gap\u001b[0;34m(self, Q, lambda_hat, nu)\u001b[0m\n\u001b[1;32m    146\u001b[0m result \u001b[39m=\u001b[39m _GapResult(L, L, L_high, gamma, error)\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m mul \u001b[39min\u001b[39;00m [\u001b[39m1.0\u001b[39m, \u001b[39m2.0\u001b[39m, \u001b[39m5.0\u001b[39m, \u001b[39m10.0\u001b[39m]:\n\u001b[0;32m--> 148\u001b[0m     _, h_hat_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_h(mul \u001b[39m*\u001b[39;49m lambda_hat)\n\u001b[1;32m    149\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39mmul=\u001b[39m\u001b[39m%.0f\u001b[39;00m\u001b[39m\"\u001b[39m, _INDENTATION, mul)\n\u001b[1;32m    150\u001b[0m     L_low_mul, _, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval(pd\u001b[39m.\u001b[39mSeries({h_hat_idx: \u001b[39m1.0\u001b[39m}), lambda_hat)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:234\u001b[0m, in \u001b[0;36m_Lagrangian.best_h\u001b[0;34m(self, lambda_vec)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_h\u001b[39m(\u001b[39mself\u001b[39m, lambda_vec):\n\u001b[1;32m    229\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Solve the best-response problem.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[39m    Returns the classifier that solves the best-response problem for\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m    the vector of Lagrange multipliers `lambda_vec`.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     classifier \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_oracle(lambda_vec)\n\u001b[1;32m    236\u001b[0m     h \u001b[39m=\u001b[39m _PredictorAsCallable(classifier)\n\u001b[1;32m    238\u001b[0m     h_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mgamma(h)\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/fairlearn/reductions/_exponentiated_gradient/_lagrangian.py:222\u001b[0m, in \u001b[0;36m_Lagrangian._call_oracle\u001b[0;34m(self, lambda_vec)\u001b[0m\n\u001b[1;32m    219\u001b[0m     estimator \u001b[39m=\u001b[39m clone(estimator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    221\u001b[0m oracle_call_start_time \u001b[39m=\u001b[39m time()\n\u001b[0;32m--> 222\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstraints\u001b[39m.\u001b[39;49mX, redY, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_weight_name: redW})\n\u001b[1;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle_execution_times\u001b[39m.\u001b[39mappend(time() \u001b[39m-\u001b[39m oracle_call_start_time)\n\u001b[1;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_oracle_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1228\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[39mif\u001b[39;00m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1224\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m > 1 does not have any effect when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is set to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1226\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs))\n\u001b[1;32m   1227\u001b[0m         )\n\u001b[0;32m-> 1228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m   1229\u001b[0m         X,\n\u001b[1;32m   1230\u001b[0m         y,\n\u001b[1;32m   1231\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m   1232\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1233\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m   1234\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1235\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m   1236\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m   1237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1238\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1239\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1240\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1241\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1242\u001b[0m     )\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m   1245\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsc80/lib/python3.8/site-packages/sklearn/svm/_base.py:1230\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1227\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1229\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1230\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1231\u001b[0m     X,\n\u001b[1;32m   1232\u001b[0m     y_ind,\n\u001b[1;32m   1233\u001b[0m     sp\u001b[39m.\u001b[39;49missparse(X),\n\u001b[1;32m   1234\u001b[0m     solver_type,\n\u001b[1;32m   1235\u001b[0m     tol,\n\u001b[1;32m   1236\u001b[0m     bias,\n\u001b[1;32m   1237\u001b[0m     C,\n\u001b[1;32m   1238\u001b[0m     class_weight_,\n\u001b[1;32m   1239\u001b[0m     max_iter,\n\u001b[1;32m   1240\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1241\u001b[0m     epsilon,\n\u001b[1;32m   1242\u001b[0m     sample_weight,\n\u001b[1;32m   1243\u001b[0m )\n\u001b[1;32m   1244\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Load and Preprocess the Data\n",
    "\n",
    "\n",
    "mrgd = mrgd.dropna()\n",
    "fare_median = mrgd['Base Passenger Fare'].median()\n",
    "mrgd['high_fare'] = (mrgd['Base Passenger Fare'] > fare_median).astype(int)\n",
    "\n",
    "features = ['Trip Distance (miles)', 'Trip Length (seconds)']\n",
    "\n",
    "ethnicities = {\n",
    "    'Black': '% Black/African American',\n",
    "    'Hispanic': '% Hispanic/Latino',\n",
    "    'White': '% White',\n",
    "    'Asian': '% Asian',\n",
    "    'Other': '% Other'\n",
    "}\n",
    "\n",
    "for eth, col in ethnicities.items():\n",
    "    median_val = mrgd[col].median()\n",
    "    mrgd[f'protected_{eth.lower()}'] = (mrgd[col] > median_val).astype(int)\n",
    "\n",
    "def compute_error_rates(y_true, y_pred, sensitive):\n",
    "    \"\"\"\n",
    "    Computes the False Positive Rate (FPR) and False Negative Rate (FNR)\n",
    "    for each subgroup defined by the sensitive attribute.\n",
    "    Returns a dictionary with keys for each group (e.g., 0 and 1).\n",
    "    \"\"\"\n",
    "    groups = np.unique(sensitive)\n",
    "    rates = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive == g)\n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        tp = np.sum((y_true_group == 1) & (y_pred_group == 1))\n",
    "        tn = np.sum((y_true_group == 0) & (y_pred_group == 0))\n",
    "        fp = np.sum((y_true_group == 0) & (y_pred_group == 1))\n",
    "        fn = np.sum((y_true_group == 1) & (y_pred_group == 0))\n",
    "        # Avoid division by zero\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "        rates[g] = {'FPR': fpr, 'FNR': fnr}\n",
    "    return rates\n",
    "\n",
    "results = []\n",
    "\n",
    "for eth, col in ethnicities.items():\n",
    "    sens_col = f'protected_{eth.lower()}'\n",
    "    print(f\"\\nEvaluating for protected attribute: {eth} ({sens_col})\")\n",
    "    \n",
    "    X = mrgd[features]\n",
    "    y = mrgd['high_fare']\n",
    "    A = mrgd[sens_col]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "        X, y, A, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # ----- Baseline Model -----\n",
    "    baseline_clf = LogisticRegression(solver='liblinear')\n",
    "    baseline_clf.fit(X_train, y_train)\n",
    "    y_pred_baseline = baseline_clf.predict(X_test)\n",
    "\n",
    "    baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "    print(f\"Baseline Accuracy: {baseline_acc:.3f}\")\n",
    "    \n",
    "    baseline_rates = compute_error_rates(y_test.values, y_pred_baseline, A_test.values)\n",
    "    baseline_fpr_disp = abs(baseline_rates[0]['FPR'] - baseline_rates[1]['FPR'])\n",
    "    baseline_fnr_disp = abs(baseline_rates[0]['FNR'] - baseline_rates[1]['FNR'])\n",
    "    \n",
    "    results.append({\n",
    "        'Ethnicity': eth,\n",
    "        'Model': 'Baseline',\n",
    "        'FPR_Disparity': baseline_fpr_disp,\n",
    "        'FNR_Disparity': baseline_fnr_disp,\n",
    "        'Accuracy': baseline_acc\n",
    "    })\n",
    "    \n",
    "    # ----- Fairness-Aware Model (ExponentiatedGradient with Equalized Odds) -----\n",
    "    constraint = EqualizedOdds()\n",
    "    fair_clf = ExponentiatedGradient(\n",
    "        estimator=LogisticRegression(solver='liblinear'),\n",
    "        constraints=constraint,\n",
    "        eps=0.01  # Adjust as needed\n",
    "    )\n",
    "    fair_clf.fit(X_train, y_train, sensitive_features=A_train)\n",
    "    y_pred_fair = fair_clf.predict(X_test)\n",
    "    \n",
    "    fair_acc = accuracy_score(y_test, y_pred_fair)\n",
    "    print(f\"Fair Model Accuracy: {fair_acc:.3f}\")\n",
    "    \n",
    "    fair_rates = compute_error_rates(y_test.values, y_pred_fair, A_test.values)\n",
    "    fair_fpr_disp = abs(fair_rates[0]['FPR'] - fair_rates[1]['FPR'])\n",
    "    fair_fnr_disp = abs(fair_rates[0]['FNR'] - fair_rates[1]['FNR'])\n",
    "    \n",
    "    results.append({\n",
    "        'Ethnicity': eth,\n",
    "        'Model': 'Fairness-Aware',\n",
    "        'FPR_Disparity': fair_fpr_disp,\n",
    "        'FNR_Disparity': fair_fnr_disp,\n",
    "        'Accuracy': fair_acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of Error Disparities and Accuracies:\")\n",
    "print(results_df)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='Ethnicity',\n",
    "    y='FPR_Disparity',\n",
    "    hue='Model',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('False Positive Rate (FPR) Disparity by Ethnicity')\n",
    "axes[0].set_ylabel('Absolute Difference in FPR')\n",
    "axes[0].set_xlabel('Ethnicity')\n",
    "\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='Ethnicity',\n",
    "    y='FNR_Disparity',\n",
    "    hue='Model',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('False Negative Rate (FNR) Disparity by Ethnicity')\n",
    "axes[1].set_ylabel('Absolute Difference in FNR')\n",
    "axes[1].set_xlabel('Ethnicity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=results_df,\n",
    "    x='Ethnicity',\n",
    "    y='Accuracy',\n",
    "    hue='Model'\n",
    ")\n",
    "plt.title('Overall Accuracy Comparison by Ethnicity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Ethnicity')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
